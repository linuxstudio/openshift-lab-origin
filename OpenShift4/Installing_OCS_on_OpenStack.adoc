= Installing OpenShift Container Storage on OpenShift 4 on OpenStack

This document describes how to install OCS on OCP 4 on OpenStack.

OpenShift Container Storage is a big application. It needs three rather large nodes - the documentation states m4.4xlarge on AWS. I was able to make it work with an `xlarge` instance flavor on our OpenStack Cluster - which has 16 vCPUs and 32Gi of memory.

Your Project Quota needs to be set to at least the following values:

* vCPUs: 100
* Instances: 10
* Volumes: 40
* Total Size of Volumes: 1000Gi
* RAM: 200000Mi

== Create OpenShift Container Storage Nodes

. Start by finding the current MachineSet.
+
[source,sh]
----
oc get machineset -n openshift-machine-api
----
+
.Sample Output
[source,texinfo]
----
NAME                 DESIRED   CURRENT   READY   AVAILABLE   AGE
f039-tks82-worker    0         0                             6d15h
general-purpose-1a   1         1         1       1           6d14h
general-purpose-1b   1         1         1       1           6d14h
infra-1a             1         1         1       1           6d14h
logging-1a           1         1         1       1           6d13h
----

. Export a MachineSet definition as YAML
+
[source,sh]
----
oc get machineset infra-1a -o yaml >ocs-1a.yaml
----

. Edit the MachineSet YAML file:
.. From metadata remove everything except `name`, `namespace` and `labels`. Change the name from `infra-1a` to `ocs-1a` (or something similar depending on your environment)
.. Change `infra-1a` to `ocs-1a` (or similar) in the rest of the file for all selectors and labels.
.. Under spec.template.spec.metadata.labels add the following label(s):
* `cluster.ocs.openshift.io/openshift-storage: ""`
.. Change the node role to
* `node-role.kubernetes.io/ocs: ""`
// .. Under spec.template.spec add the taints `node.ocs.openshift.io/storage="true":NoSchedule` to keep non OCS pods off these nodes (make sure to put `true` in double quotes).
.. Set the instance `flavor` to `12c32g30d` (12 vCPUs, 32Gi memory).
.. Set the number of replicas to 3.
.. Leave everything else the way it is.
+
[NOTE]
The label `cluster.ocs.openshift.io/openshift-storage: ""`
// and the taint `node.ocs.openshift.io/storage=true:NoSchedule` are 
is usually set by the OCS Deployment Wizard in the web console. But we can not use that wizard because we need to modify the Custom Resource for the storage cluster.
+
. Example of completed MachineSet:
[source,yaml]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: f039-tks82
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: ocs-1a
  namespace: openshift-machine-api
spec:
  replicas: 3
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: f039-tks82
      machine.openshift.io/cluster-api-machineset: ocs-1a
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: f039-tks82
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: ocs-1a
    spec:
      metadata:
        labels:
          failure-domain.beta.kubernetes.io/region: regionOne
          failure-domain.beta.kubernetes.io/zone: nova
          node-role.kubernetes.io/ocs: ""
          cluster.ocs.openshift.io/openshift-storage: ""
      providerSpec:
        value:
          apiVersion: openstackproviderconfig.openshift.io/v1alpha1
          cloudName: openstack
          cloudsSecret:
            name: openstack-cloud-credentials
            namespace: openshift-machine-api
          flavor: 12c32g30d
          image: rhcos-ocp42
          kind: OpenstackProviderSpec
          networks:
          - filter: {}
            subnets:
            - filter:    
                name: f039-ocp-subnet
          securityGroups:
          - filter: {}
            name: f039-worker_sg
          serverMetadata:
            Name: f039-tks82-worker
            openshiftClusterID: f039-tks82
          tags:
          - openshiftClusterID=f039-tks82
          trunk: true
          userDataSecret:
            name: worker-user-data
----
//       taints:
//       - effect: NoSchedule
//         key: node.ocs.openshift.io/storage
//         value: "true"
// ----

. Create the MachineSet
+
[source,sh]
----
oc apply -f ocs-1a.yaml
----

. Wait until the Machines are `ACTIVE`
+
[source,sh]
----
oc get machines -n openshift-machine-api
----
+
.Sample Output
[source,texinfo]
----
NAME                       STATE    TYPE       REGION      ZONE   AGE
f039-tks82-master-0        ACTIVE   4c16g30d   regionOne   nova   6d15h
f039-tks82-master-1        ACTIVE   4c16g30d   regionOne   nova   6d15h
f039-tks82-master-2        ACTIVE   4c16g30d   regionOne   nova   6d15h
general-purpose-1a-tfdjm   ACTIVE   4c12g30d   regionOne   nova   6d14h
general-purpose-1b-mrkvh   ACTIVE   4c12g30d   regionOne   nova   6d14h
infra-1a-fl9vp             ACTIVE   4c12g30d   regionOne   nova   6d14h
logging-1a-m2bfk           ACTIVE   4c16g30d   regionOne   nova   6d14h
ocs-1a-9krk5               ACTIVE   8c32g30d   regionOne   nova   3m17s
ocs-1a-nqhh2               ACTIVE   8c32g30d   regionOne   nova   3m17s
ocs-1a-qh6vx               ACTIVE   8c32g30d   regionOne   nova   4m17s
----
+
If any Machines are in error check the machine, machineset, events and the `machine-api-controllers` pod logs. You may need to increate the Quota for the cluster.

. Validate that your Nodes are `Ready` (this may take 5-6 minute).
+
[source,sh]
----
oc get nodes
----
+
.Sample Output
[source,texinfo]
----
NAME                       STATUS   ROLES                AGE     VERSION
f039-tks82-master-0        Ready    master               6d15h   v1.14.6+888f9c630
f039-tks82-master-1        Ready    master               6d15h   v1.14.6+888f9c630
f039-tks82-master-2        Ready    master               6d15h   v1.14.6+888f9c630
general-purpose-1a-tfdjm   Ready    general-use,worker   6d14h   v1.14.6+888f9c630
general-purpose-1b-mrkvh   Ready    general-use,worker   6d14h   v1.14.6+888f9c630
infra-1a-fl9vp             Ready    infra,worker         6d14h   v1.14.6+888f9c630
logging-1a-m2bfk           Ready    logging,worker       6d14h   v1.14.6+888f9c630
ocs-1a-9krk5               Ready    ocs,worker           49s     v1.14.6+888f9c630
ocs-1a-nqhh2               Ready    ocs,worker           21s     v1.14.6+888f9c630
ocs-1a-qh6vx               Ready    ocs,worker           101s    v1.14.6+888f9c630
----

== Deploy OpenShift Container Storage Operator

. Create the namespace `openshift-storage`.
+
[source,sh]
----
cat << 'EOF' >$HOME/ocs-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-storage
  labels:
    openshift.io/cluster-monitoring: "true"
spec: {}
EOF

oc create -f $HOME/ocs-namespace.yaml
----

. Create the OperatorGroup.
+
[source,sh]
----
cat << 'EOF' >$HOME/rhocs-operatorgroup.yaml
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-storage-operatorgroup
  namespace: openshift-storage
spec:
  serviceAccount:
    metadata:
      creationTimestamp: null
  targetNamespaces:
  - openshift-storage
EOF

oc create -f $HOME/rhocs-operatorgroup.yaml
----

. Follow the instructions at https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.2/html/deploying_openshift_container_storage/deploying-openshift-container-storage#installing-openshift-container-storage-operator-using-the-operator-hub_rhocs to deploy the OpenShift Container Storage Operator into the `openshift-storage` project.
+
[IMPORTANT]
*Do not deploy the Storage Cluster from the OLM.*
+
// . The operators can be deployed by running the following command.
// +
// [source,sh]
// ----
// oc apply -f https://raw.githubusercontent.com/openshift/ocs-operator/release-4.2/deploy/deploy-with-olm.yaml
// ----
// +
// .Sample Output
// [source,texinfo]
// ----
// namespace/openshift-storage created
// namespace/local-storage created
// operatorgroup.operators.coreos.com/openshift-storage-operatorgroup created
// operatorgroup.operators.coreos.com/local-operator-group created
// catalogsource.operators.coreos.com/local-storage-manifests created
// catalogsource.operators.coreos.com/ocs-catalogsource created
// subscription.operators.coreos.com/ocs-subscription created
// ----

. Switch to the `openshift-storage` project.
+
[source,sh]
----
oc project openshift-storage
----

. Wait until the Cluster Service Version show `Succeeded` as the status.
+
[source,sh]
----
oc get csv
----
+
.Sample Output
[source,texinfo,options="nowrap"]
----
NAME                                         DISPLAY                       VERSION               REPLACES                                     PHASE
elasticsearch-operator.4.2.14-202001061701   Elasticsearch Operator        4.2.14-202001061701   elasticsearch-operator.4.2.13-201912230557   Succeeded
ocs-operator.v4.2.0                          OpenShift Container Storage   4.2.0                                                              Succeeded
----

. Validate that your Operator pods are running:
+
[source,sh]
----
oc get pod -n openshift-storage
----
+
.Sample Output
[source,texinfo]
----
NAME                                  READY   STATUS    RESTARTS   AGE
noobaa-operator-d8f77bfd9-tmzwp       1/1     Running   0          92s
ocs-operator-6b9f889bb9-f7db6         1/1     Running   0          92s
rook-ceph-operator-69c8dd848b-nszc8   1/1     Running   0          92s
----

. Create the `StorageCluster` YAML Manifest.
+
[source,sh]
----
cat << 'EOF' >$HOME/ocs.yaml
apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  manageNodes: false
  storageDeviceSets:
  - name: ocs-deviceset
    count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 15Gi
        storageClassName: "standard"
        volumeMode: Block
    placement: {}
    portable: true
    replica: 3
    resources: {}
EOF
----
+
[NOTE]
There are two changes compared to when you would have created this via the Operator Management in the OpenShift console. We are setting the `storageClassName` to `standard` (it is empty by default) and we are changing the storage request from `1Ti` to `15Gi`. Without these changes the deployment would fail.

. Create the StorageCluster.
+
[source,sh]
----
oc create -f $HOME/ocs.yaml
----

. This will create the entire storage system. This will take a while. Watch the pods until every pod is running and ready. The final state should look similar to this:
+
[source,sh]
----
watch oc get pod -n openshift-storage
----
+
.Sample Output
[source,texinfo]
----
NAME                                                              READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-2qrpp                                            3/3     Running     0          6m14s
csi-cephfsplugin-9bj5x                                            3/3     Running     0          6m15s
csi-cephfsplugin-d2nbz                                            3/3     Running     0          6m15s
csi-cephfsplugin-fwbjv                                            3/3     Running     0          6m15s
csi-cephfsplugin-provisioner-5b9fc6bc65-m8jkv                     4/4     Running     0          6m15s
csi-cephfsplugin-provisioner-5b9fc6bc65-z97sk                     4/4     Running     0          6m15s
csi-cephfsplugin-r8nnb                                            3/3     Running     0          6m15s
csi-rbdplugin-kchk8                                               3/3     Running     0          6m15s
csi-rbdplugin-nvh4d                                               3/3     Running     0          6m15s
csi-rbdplugin-provisioner-f846955f5-sktks                         4/4     Running     0          6m15s
csi-rbdplugin-provisioner-f846955f5-xzmk5                         4/4     Running     0          6m15s
csi-rbdplugin-r2qft                                               3/3     Running     0          6m15s
csi-rbdplugin-v56fq                                               3/3     Running     0          6m15s
csi-rbdplugin-xdllz                                               3/3     Running     0          6m15s
noobaa-core-0                                                     2/2     Running     0          2m20s
noobaa-operator-854756dff7-6vhzl                                  1/1     Running     0          6m54s
ocs-operator-76b75cb5d9-9g8g6                                     1/1     Running     0          6m54s
rook-ceph-drain-canary-ocs-1a-plcgm-7fd767444d-jw4n6              1/1     Running     0          2m26s
rook-ceph-drain-canary-ocs-1a-szr6p-7dd6675f95-ltmf4              1/1     Running     0          2m27s
rook-ceph-drain-canary-ocs-1a-xdfdd-6b44d4b94d-896z4              1/1     Running     0          2m34s
rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-7bdf75bb4x27b   1/1     Running     0          2m13s
rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-6549dbdb42xbd   1/1     Running     0          2m13s
rook-ceph-mgr-a-5c56bf48f6-sc8qk                                  1/1     Running     0          3m25s
rook-ceph-mon-a-7bd94c6494-cgzgq                                  1/1     Running     0          5m3s
rook-ceph-mon-b-76447c44cf-hpf2l                                  1/1     Running     0          4m38s
rook-ceph-mon-c-7b5755dfd8-6gv28                                  1/1     Running     0          3m57s
rook-ceph-operator-dd7759478-4pwh2                                1/1     Running     0          6m54s
rook-ceph-osd-0-5b9f48cf89-2hd7d                                  1/1     Running     0          2m34s
rook-ceph-osd-1-546bd6c6c7-ckqp9                                  1/1     Running     0          2m27s
rook-ceph-osd-2-6c759fcc99-46bk6                                  1/1     Running     0          2m26s
rook-ceph-osd-prepare-ocs-deviceset-0-0-mw65m-69hn2               0/1     Completed   0          3m1s
rook-ceph-osd-prepare-ocs-deviceset-1-0-m4jfn-vgwwl               0/1     Completed   0          3m1s
rook-ceph-osd-prepare-ocs-deviceset-2-0-5lwgg-7st55               0/1     Completed   0          3m1s
rook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-878f9999k964   1/1     Running     0          114s
----

. OpenShift Container Storage is ready to be used.
. Validate the 3 new Storage Classes:
+
[source,sh]
----
oc get sc
----
+
.Sample Output
[source,texinfo]
----
NAME                          PROVISIONER                             AGE
ocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      5h27m
ocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   5h27m
openshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         80s
standard (default)            kubernetes.io/cinder                    6d20h
----

== Accessing the Storage Dashboards

. In your OpenShift Web Console select *Home* and then *Dashboards* in the navigator on the left.
+
You will see that there are additional dashboards available now: *Persistent Storage* and *Object Service*
. Explore these dashboards.
. From the *Object Service* Dashboard open the *Noobaa* dashboard by clicking on the *noobaa* link (you have to open that link in a Chrome compatible browser).


// == Accessing the NooBaa Dashboard

// . Retrieve the Noobaa route:
// +
// [source,sh]
// ----
// oc get route -n openshift-storage
// ----
// +
// .Sample Output
// [source,texinfo]
// ----
// NAME          HOST/PORT                                                      PATH   SERVICES      PORT         TERMINATION   WILDCARD
// noobaa-mgmt   noobaa-mgmt-openshift-storage.apps.f039.blue.osp.opentlc.com          noobaa-mgmt   mgmt-https   reencrypt     None
// s3            s3-openshift-storage.apps.f039.blue.osp.opentlc.com                   s3            s3-https     reencrypt     None
// ----

// . Use the created Route (in a Chrome compatible browser) to open the NooBaa web interface. Use a cluster-admin user to log into NooBaa.

== Conclusion

This concludes this lab. You now have OpenShift Container Storage deployed on your cluster and ready to use.
